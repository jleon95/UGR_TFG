\pdfbookmark[1]{Resumen}{Resumen}
\begingroup
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Resumen}

Gracias a los avances de los últimos años en potencia de cómputo, el campo de las redes neuronales artificiales ha tenido un importante resurgimiento que ha traído grandes progresos en muchos problemas que se creían demasiado difíciles. Sin embargo, esta potencia renovada trae consigo un incremento en la dificultad de encontrar la configuración más apropiada para cada problema en concreto.

En este trabajo se propone un método evolutivo incremental para optimizar hiperparámetros en redes neuronales. Tras una fase previa de selección de características, se hace evolucionar una primera población de redes neuronales para encontrar arquitecturas que proporcionen un rendimiento mayor al inicial. Después, en una segunda fase se hace evolucionar otra población con el objetivo de encontrar la mejor combinación de ratio de aprendizaje, tasa de \textit{dropout} y épocas de entrenamiento. Adicionalmente, en cada una de estas fases es posible aprovechar o bien el paralelismo implícito de una GPU o bien el paralelismo a nivel de distribución de tareas a distintas CPUs.

En su aplicación a clasificación en visualización motora usando interfaces cerebro-máquina, este método parece conseguir resultados muy prometedores en cuanto a precisión, usando tanto redes neuronales como clasificadores más simples como SVM.\\

\textbf{Palabras clave:} Interfaces cerebro-máquina (BCI) · Algoritmos genéticos · Redes neuronales artificiales · Selección de características · Optimización de hiperparámetros