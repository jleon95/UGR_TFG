\chapter{Experimental results}\label{ch:experiments}
%************************************************ 

In this chapter, we start by detailing in section \ref{sec:technologies} the choice of technologies in order to obtain experimental results. After that, these results will be discussed in order of application in sections \ref{sec:res_fs} (feature selection), \ref{sec:res_so} (structure optimization) and \ref{sec:res_lo} (learning optimization). By the end, we will have empirical evidence to point us in promising directions, which we will subsequently address when we talk about conclusions and future work.

\section{Software and hardware}\label{sec:technologies}

	\subsection{Software}

		The first decision to make is which programming language to use. \texttt{Python} is the choice for the following reasons:

		\begin{itemize}

			\item
			Previous experience with the language in web and machine learning applications.

			\item
			Popularity of the language, which is a good indicator of community support. According to the \textit{StackOverflow} 2018 Survey \footnote{\href{https://insights.stackoverflow.com/survey/2018/\#technology-programming-scripting-and-markup-languages}{\textit{StackOverflow} 2018 Survey: Most popular technologies}}, it is one of the most popular languages, and more so if we compare it with those commonly associated with machine learning in the last few years.

			\item
			Popularity of its machine learning and deep learning frameworks. Well-established frameworks include \texttt{Scikit-learn} \footnote{\href{https://github.com/scikit-learn/scikit-learn}{\texttt{Scikit-learn} GitHub repository}}, \\ \texttt{Caffe} \footnote{\href{https://github.com/BVLC/caffe}{\texttt{Caffe} GitHub repository}}, \texttt{TensorFlow} \footnote{\href{https://github.com/tensorflow/tensorflow}{\texttt{TensorFlow} GitHub repository}} and \texttt{Theano} \footnote{\href{https://github.com/Theano/Theano}{\texttt{Theano} GitHub repository}}. The last two of them also function as backends for the high-level neural networks API \texttt{Keras} \footnote{\href{https://github.com/keras-team/keras}{\texttt{Keras} GitHub repository}}. If we look at the number of stars in their \textit{GitHub} repositories, we can see that they are widely acknowledged by the community.

		\end{itemize}

\newpage

		The next step is choosing the tools to support our work. Since one of the goals of this project was to learn about optimization techniques, the genetic algorithm has been implemented from scratch, although there are alternatives like \texttt{DEAP} \footnote{\href{https://github.com/DEAP/deap}{\texttt{DEAP} GitHub repository}} if one wishes to avoid the additional development effort.

		Data operations become faster and easier with \texttt{Numpy} \footnote{\href{https://github.com/numpy/numpy}{\texttt{Numpy} GitHub repository}}. This will allow us to manage populations in genetic algorithms, as well as perform basic operations in a vectorized way whenever we need them. It is also fully compatible with the other libraries.

		Building machine learning models from scratch too is understandably out of the question. For this reason, we will rely on \texttt{Scikit-learn} for general machine learning algorithms and metrics, and on \texttt{Keras}---with its default \texttt{TensorFlow} backend---for neural networks.

		Lastly, many charts will be generated using \texttt{R} \footnote{\href{https://www.r-project.org/}{\texttt{R} Project website}}, which provides simple and powerful functionality for this task.

	\subsection{Hardware}

		We can make a distinction in this regard between the main development system, used for testing and debugging, and the dedicated servers for full-scale experimentation:

		\begin{itemize}

			\item
			Development system:

			\begin{itemize}

				\item
				Intel® Core™ i5-3470 CPU @ 3.20GHz, 8GB DDR3.
				\item
				NVIDIA GeForce® GTX 960, 2GB GDDR5.

			\end{itemize}

			\item
			First dedicated server:

			\begin{itemize}

				\item
				Intel® Xeon® E5-2620 v2 @ 2.10GHz, 32GB DDR3.
				\item
				NVIDIA Tesla® K20c, 5GB GDDR5.

			\end{itemize}

			\item
			Second dedicated server:

			\begin{itemize}

				\item
				Intel® Xeon® E5-2620 v4 @ 2.10GHz, 32GB DDR4.
				\item
				NVIDIA Tesla® K40m, 12GB GDDR5.

			\end{itemize}

			\item
			Third dedicated server:

			\begin{itemize}

				\item
				Two Intel® Xeon® E5-2620 v4 @ 2.10GHz, 32GB DDR4.
				\item
				NVIDIA Tesla® K40m, 12GB GDDR5.

			\end{itemize}

		\end{itemize}

\newpage

\section{Feature selection}\label{sec:res_fs}

	Determining the right configuration for the genetic algorithm---or rather, even one that is good enough---is no trivial task. Early experimentation seemed to point to a high crossover probability, but especially to a high mutation probability (ultimately set to 1). We will elaborate on that soon.

	Let us start by comparing three different crossover operators: \textit{Uniform}, \textit{Single-point} and \textit{Two-point}. We will work with a population of 300 individuals and 150 generations. We will set a 0.9 crossover probability after which a mutation will always ensue. A maximum of 50 active features will be allowed in each individual. The fitness criteria will take into account the Kappa value (for test accuracy) and a 5-fold cross-validation (for generalization assessment) measured for Logistic Regression.

	We will do an initial test run on each subject (104, 107 and 110). 

	Figure \ref{gfx:fs_crossover_kappa} displays the evolution of the mean Kappa error for all crossover operators in all three subjects.

    \begin{figure}[h]

        \begin{center}

        	\setlength{\fboxrule}{0pt}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_Kappa_104.png}
				\end{varwidth}
			}
			\fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_Kappa_107.png}
				\end{varwidth}
			}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_Kappa_110.png}
				\end{varwidth}
			}

		\end{center}
		\caption[Kappa loss comparison for different crossovers]{Comparison of Kappa loss evolution over time with different crossover operators.}\label{gfx:fs_crossover_kappa}

	\end{figure}

	We can identify appreciable differences between the three curves: the blue one (uniform crossover) shows consistently better results than the other two, and the red one (single-point crossover) appears to be the worst.

\newpage

	Let us move on now to the same type of chart but with the cross-validation error (Figure \ref{gfx:fs_crossover_cv}). Again, the uniform crossover operator achieves the top performance across all individuals and the single-point crossover operator often lags behind.

	\begin{figure}[h]

        \begin{center}

        	\setlength{\fboxrule}{0pt}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_CV_104.png}
				\end{varwidth}
			}
			\fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_CV_107.png}
				\end{varwidth}
			}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_Crossover_CV_110.png}
				\end{varwidth}
			}

		\end{center}
		\caption[CV loss comparison for different crossovers]{Comparison of cross-validation loss evolution over time with different crossover operators.}\label{gfx:fs_crossover_cv}

	\end{figure}

	We seem to be spotting a trend that depends on the crossover operator. However, is it wise to extrapolate from only one test run? The answer is no: the running times allow us to repeat the experiment several times and find out whether their differences are statistically significant.

	As a compromise between quantity of samples and time expended, we will analyze 15 samples per crossover operator using their Kappa error values. Table \ref{table:crossover_kappa} shows the resulting values for all possible combinations:

	\vspace{0.3cm}

	\begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.5in} | >{\centering\arraybackslash}m{1.1in} |  >{\centering\arraybackslash}m{1.1in} | >{\centering\arraybackslash}m{1.1in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{Subject} & \textbf{Uniform} & \textbf{Single-point} & \textbf{Two-point} \\
            \hline
            \textbf{104} & $0.06534 \pm 0.0074$ & $0.08619 \pm 0.0102$ & $0.07941 \pm 0.0119$ \\
            \hline
            \textbf{107} & $0.15202 \pm 0.0132$ & $0.18004 \pm 0.0137$ & $0.17725 \pm 0.0150$ \\
            \hline
            \textbf{110} & $0.14829 \pm 0.0138$ & $0.17131 \pm 0.0158$ & $0.16569 \pm 0.0187$ \\
            \hline

        \end{tabular}

        \caption{Comparison of average Kappa error values for the three subjects and the three crossover operators.}\label{table:crossover_kappa}

    \end{table}

    The average performance of each operator seems to be what we expected. Additionally, the uniform crossover appears to produce slightly more stable results, judging from the standard deviation. Next, and not making assumptions about normality, we will perform a \textit{Kruskal-Wallis} test to see if their differences are worth considering. The \textit{p-values} are displayed in Tables \ref{table:crossover_kruskal_104}, \ref{table:crossover_kruskal_107} and \ref{table:crossover_kruskal_110}, with values below $0.05$ representing meaningful differences ($95\%$ confidence interval).

	\begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{104} & \textbf{Single-point} & \textbf{Two-point} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Uniform} & $p = 0.000027$ & $p = 0.000835$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Single-point} & \cellcolor{lightgray} & \textcolor{red}{$p = 0.056282$} \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the crossover operators (subject 104).}\label{table:crossover_kruskal_104}

    \end{table}

    \begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{107} & \textbf{Single-point} & \textbf{Two-point} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Uniform} & $p = 0.000023$ & $p = 0.000104$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Single-point} & \cellcolor{lightgray} & \textcolor{red}{$p = 0.678133$} \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the crossover operators (subject 107).}\label{table:crossover_kruskal_107}

    \end{table}

    \begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{110} & \textbf{Single-point} & \textbf{Two-point} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Uniform} & $p = 0.000454$ & $p = 0.006561$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{Single-point} & \cellcolor{lightgray} & \textcolor{red}{$p = 0.299489$} \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the crossover operators (subject 110).}\label{table:crossover_kruskal_110}

    \end{table}

    As we can see, the single-point and two-point crossovers show no statistically significant differences in their recorded results.

    The computational impact of either operator is negligible against the much more intensive fitness evaluations, so there is no reason not to consider the uniform crossover as our pick from now on. The last thing to do before moving on is attempt to explain why this happens. 

    On one hand, $n$-point crossovers merge large blocks from both parents; this leads to a not-so-optimal information transfer when trying to pick a handful of features from a pool of 3600.

    On the other hand, the uniform crossover is often said to be very disruptive, due to randomly choosing elements for which the parents do not agree. However, when only a small portion $k$ of features is active at a time, the disruption is at most $2k$ elements (the rest are inactive features); on top of that, features in which both parents agree are always kept. This makes the uniform crossover excel at \textit{exploitation} while also helping in \textit{exploration}. Together with frequent mutations, this is probably the key of the performance gain of the algorithm.

    Another aspect to look into is the effect of population sizes and number of generations. Bigger populations should introduce greater exploration possibilities, while more generations should allow the genetic algorithm to converge to even better solutions. Nevertheless, an increase in these parameters has a direct influence in computation times, so we will have to check if the improvement is worth the effort.

    We were working with a population size of 300 individuals and a number of generations equal to 150. The first alternative we propose is a tradeoff between a bigger population and less generations: 500 and 100. The second alternative is an increase in both: 800 individuals and 200 generations. Like we did before, we will start by observing what happens in a single run. In Figure \ref{gfx:fs_popgen_kappa} we can take a look at the behavior of the genetic algorithm in terms of Kappa loss.

    \vspace{0.3cm}

    \begin{figure}[h]

        \begin{center}

        	\setlength{\fboxrule}{0pt}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_Kappa_104.png}
				\end{varwidth}
			}
			\fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_Kappa_107.png}
				\end{varwidth}
			}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_Kappa_110.png}
				\end{varwidth}
			}

		\end{center}
		\caption[Kappa loss comparison for different populations and generations]{Comparison of Kappa loss evolution over time with different configurations of population and generations.}\label{gfx:fs_popgen_kappa}

	\end{figure}

	There is one definite conclusion we can draw: the number of generations is preventing the emergence of better individuals. The curves follow similar paths until they are progressively stopped by the generation limit. If we take a look at Figure \ref{gfx:fs_popgen_cv}, the same phenomenon is taking place for cross-validation loss, which does not surprise us.

\newpage

	\begin{figure}[bth]

        \begin{center}

        	\setlength{\fboxrule}{0pt}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_CV_104.png}
				\end{varwidth}
			}
			\fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_CV_107.png}
				\end{varwidth}
			}
            \fbox{
				\begin{varwidth}{\textwidth}
					\centering
					\includegraphics[width=0.45\textwidth]{gfx/FS_IndGens_CV_110.png}
				\end{varwidth}
			}

		\end{center}
		\caption[Cross-validation loss comparison for different populations and generations]{Comparison of cross-validation loss evolution over time with different configurations of population and generations.}\label{gfx:fs_popgen_cv}

	\end{figure}

	It is clear that we need a relevant analysis like that of crossover operators. We will keep the number of individuals intact to grant more diversity, but we have to confirm the importance of extending the evolutionary process. For all this, another 15 algorithm runs for each alternative will be used to make statistical claims.

	First, let us put the average performances alongside one another in Table \ref{table:popgen_kappa}:

	\vspace{0.3cm}

	\begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.5in} | >{\centering\arraybackslash}m{1.1in} |  >{\centering\arraybackslash}m{1.1in} | >{\centering\arraybackslash}m{1.1in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{Subject} & \textbf{300-150} & \textbf{500-100} & \textbf{800-200} \\
            \hline
            \textbf{104} & $0.06534 \pm 0.0074$ & $0.06310 \pm 0.0077$ & $0.05127 \pm 0.0067$ \\
            \hline
            \textbf{107} & $0.15202 \pm 0.0132$ & $0.16830 \pm 0.0145$ & $0.12122 \pm 0.0126$ \\
            \hline
            \textbf{110} & $0.14829 \pm 0.0138$ & $0.15448 \pm 0.0069$ & $0.13369 \pm 0.0083$ \\
            \hline

        \end{tabular}

        \caption{Comparison of average Kappa error values for the three subjects and the three configurations.}\label{table:popgen_kappa}

    \end{table}

    We see that taking the population size up to 800 and the generations up to 200 yields finer average results. It is also noticeable that a bigger population appears to matter less when the number of generations is not accordingly increased.

\newpage

    Tables \ref{table:popgen_kruskal_104}, \ref{table:popgen_kruskal_107} and \ref{table:popgen_kruskal_110} show the Kruskal-Wallis p-values for every pair of combinations. Again, we work with a $95\%$ confidence interval, which means that values below $0.05$ point to statistically significant differences.

    \vspace{0.3cm}

    \begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{104} & \textbf{300-150} & \textbf{500-100} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{800-200} & $p = 0.000144$ & $p = 0.000301$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{300-150} & \cellcolor{lightgray} & \textcolor{red}{$p = 0.884484$} \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the evolutionary configurations (subject 104).}\label{table:popgen_kruskal_104}

    \end{table}

    \begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

        	\hline
            \rowcolor{RoyalBlue}
            \textbf{107} & \textbf{300-150} & \textbf{500-100} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{800-200} & $p = 0.000016$ & $p = 0.000005$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{300-150} & \cellcolor{lightgray} & $p = 0.002441$ \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the evolutionary configurations (subject 107).}\label{table:popgen_kruskal_107}

    \end{table}

    \begin{table}[h]

        \centering
        \setlength\arrayrulewidth{0.8pt}

        \begin{tabular}{| >{\centering\arraybackslash}m{0.9in} |  >{\centering\arraybackslash}m{0.9in} | >{\centering\arraybackslash}m{0.9in} |}

            \hline
            \rowcolor{RoyalBlue}
            \textbf{110} & \textbf{300-150} & \textbf{500-100} \\
            \hline
            \cellcolor{RoyalBlue}\textbf{800-200} & $p = 0.003392$ & $p = 0.000005$ \\
            \hline
            \cellcolor{RoyalBlue}\textbf{300-150} & \cellcolor{lightgray} & $p = 0.034037$ \\
            \hline

        \end{tabular}

        \caption{Comparison of p-values for the evolutionary configurations (subject 110).}\label{table:popgen_kruskal_110}

    \end{table}

    The first thing we notice is that the combination 800-200 is consistently different from the other two. The differences between 300-150 and 500-100 is not so clear at times (see individual 104), but we could say that in a general case the approach with more generations can arrive at better solutions.

    We find ourselves in a quandary between prioritizing computation times or results. The running times are certainly higher with 800-200, but they are still under 2 or 3 hours, which means we can do a lot in a single day. On the side of quality, an improvement of just a few percentage points at this level can be invaluable. Given all this, we will keep the 800-200 configuration and make it our baseline for our forthcoming comparisons against neural networks.

%\section{Structure optimization}\label{sec:res_so}

%\section{Learning optimization}\label{sec:res_lo}