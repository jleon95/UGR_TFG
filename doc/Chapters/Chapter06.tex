\chapter{Neural network optimization}\label{ch:optimization}
%************************************************

The definition of hyperparameter is often fuzzy. In this work, we view hyperparameters as those parameters which are fixed before training a model, because there are no direct rules to infer them from the available data. In fact, we probably would not need them at all if we had enough data, since the samples would tell us everything that is to be known about a given problem. As we will almost never find ourselves in such a favorable situation, there exists a real need to find the right combination of hyperparameters for the circumstances at hand.

One could understand feature selection as a form of hyperparameter optimization, since it is in a way fixing a part of the model---the entry point of the data. Although it is undoubtedly a key tool when dealing with an absurd quantity of decision variables, whether it is hyperparameter optimization or not, it already has a section in its own right. From here on, we will focus on tuning aspects specific to neural networks, such as the overall structure, the learning rate, the number of epochs in training or the dropout rate \cite{srivastava2014dropout}.

Perhaps the most common technique for this task is the \textit{grid search}. In grid search, we manually specify a set of values for each hyperparameter, and then the algorithm tries every possible combination. While efficient in low-dimensional spaces, it can also suffer from the curse of dimensionality. However, it is \textit{embarrassingly parallel} in exchange, for the combinations are independent from one another.

When the above method becomes too resource-intensive, random search \cite{bergstra2012random} comes into play. It was recently shown that a random search is capable of achieving similar results within a fraction of the time otherwise consumed by a grid equivalent; furthermore, it could yield even better outcomes if granted the same computation power.

Finally, if we look at quality-guided searches, we find classic algorithms such as \textit{simulated annealing} \cite{pai2005support}, \textit{particle swarm optimization} \cite{lin2008particle} or, again, genetic algorithms \cite{leung2003tuning}.

For scope constraints we will make use of genetic algorithms as our main optimization technique, carrying on our \acs{NSGA-II} implementation from the feature selection phase. In general, it is computationally more expensive than random search---future work could tackle the problem from this point of view---, but in turn it has the genetic operators to leverage.

In the following two sections we will describe two separate optimizations: first, we will find an approximate structure (hidden layers and their sizes); then, we will tune the learning and dropout rates and the number of training epochs. The reason for this is that their nature is different---the former deals with a variable number of parameters that define a whole, whereas the latter comprises exactly three parameters that are not conceptually linked together. Besides, joining these two parts would entail an overly convoluted search process.

Now, let us begin without further delay. We will reuse the \acs{NSGA-II} framework (Algorithm \ref{alg:nsga}), as well as Algorithms \ref{alg:evaluation}, \ref{alg:nds}, \ref{alg:fronts}, \ref{alg:crowding_distance}, \ref{alg:selection} and \ref{alg:replacement}.

\section{Structure optimization}

	We define the structure of a neural network as its configuration of layers and units per layer. To find out which combination is more appropriate, we will input a maximum number of layers---fixing the other parameters so that comparisons are fair---and the genetic algorithm will give us its rough estimation of what works best.

	The only change with respect to Algorithm \ref{alg:nsga} is that it will receive the input size and the maximum number of hidden layers instead of the maximum number of features.

	The population initialization will create a heterogeneous set of neural network structures which share one trait: they are widest in the middle layer and narrower towards the first and last layers. From this point on, the evolution has freedom to converge to other shapes.

	\vspace{0.2cm}

	\begin{algorithm}

		\Fn{Initialize}{

			\KwIn{population size, input size, max hidden}
			\KwOut{population}
			population $\longleftarrow$ $\emptyset$\;
			sizes $\longleftarrow$ RandomChoice($1$, max hidden, population size)\;
			\For{$i = 0$ \KwTo $population$ $size$}{
				ind $\longleftarrow$ EmptyVector(sizes[i])\;
				ind[$0$] $=$ RandomChoice(input size, $1.75$*input size, $1$)\;
				\If{$sizes[i] > 1$}{
					middle $=$ sizes[i]/$2$ + $1$\;
					\For{$j = 1$ \KwTo $middle$}{
						ind[j] $=$ ind[j-$1$] + RandomChoice(ind[j-$1$], $1$)\;
					}
					\For{$j = middle$ \KwTo $sizes[i]$}{
						ind[j] $=$ ind[j-$1$] - RandomChoice(ind[j-$1$], $1$)\;
					}
				}
				population $\longleftarrow$ population $\cup$ $\{$ind$\}$\;
			}

			\KwRet population\;
		}

		\caption{Initialization of a population of structures}

	\end{algorithm}

	\vspace{0.3cm}

	The initialization also has to consider the case where one or more layers in the second half reach a null or negative unit count. The correction will depend on the particular implementation.

	Another procedure that undergoes slight modifications is the offspring creation. Given that structures are highly cohesive wholes, the crossover is not as useful and so the mutation takes a step forward in relevance: it is now independent from a successful crossover taking place, and contributes directly to the offspring pool (compare with Algorithm \ref{alg:offspring_fs}):

	\vspace{0.3cm}

	\begin{algorithm}[H]\label{alg:offspring_nn}

		\Fn{CreateOffspring}{

			\KwIn{parents}
			\KwOut{offspring}
			offspring $\longleftarrow$ $\emptyset$\;
			\For{$i = 0$ \KwTo \textit{size(parents)}}{
				\If{\textit{Random()} $\leq$ \textit{crossover probability}}{
					$p_1$, $p_2$ = RandomChoice(parents, $2$)\;
					child $\longleftarrow$ Crossover($p_1$,$p_2$)\;
				}
				\ElseIf{\textit{Random()} $\leq$ \textit{mutation probability}}{
					$p$ = RandomChoice(parents, $1$)\;
					child $\longleftarrow$ Mutation(p)\;
				}
				offspring $\longleftarrow$ offspring $\cup$ $\{$child$\}$\;

			}
			\KwRet offspring\;
		}

		\caption{Offspring creation in structure optimization}

	\end{algorithm}

	\vspace{0.3cm}

	Notice that the size of the offspring population is now determined by the sum of the two probabilities. The mutation will typically occur more often, thus making the algorithm resemble a random search with guiding criteria.	Also, to improve the exploration we could have different types of mutation and alternate between them at random.

	In Algorithms \ref{alg:layerx}, \ref{alg:layerm} and \ref{alg:scalem} we can see the possibilities at our disposal at the time of writing.

	The first one, \textit{Midpoint crossover}, takes the ascending half of one parent and joins it with the descending half of the other parent.

	The second one is the \textit{Single Layer mutation}, which alters the number of units of a given layer and compensates the change by distributing the opposite operation among the other layers. It uses a magnitude modifier and a value from a normal distribution. This makes it a shape mutation rather than a size mutation.

	The third one consists in scaling the whole network evenly by adding or subtracting a certain number of neurons at every layer---hence its name, \textit{Uniform Scaling mutation}.

	\vspace{0.3cm}

	\begin{algorithm}[H]\label{alg:layerx}

		\Fn{MidPoint}{

			\KwIn{$p_1$, $p_2$}
			\KwOut{child}
			m = FindMiddleLayer($p_1$)\;
			n = FindMiddleLayer($p_2$)\;
			child $\longleftarrow$ $\emptyset$\;
			\For{$i = 0$ \KwTo $m$}{
				child[i] $=$ $p_1$[i]\;
			}
			\For{$i = 0$ \KwTo \textit{size($p_2$)} $-$ $n$}{
				child[m+i] $=$ $p_2$[n+i]\;
			}
			\KwRet child\;
		}

		\caption{Midpoint crossover}

	\end{algorithm}

	\vspace{0.3cm}

	\begin{algorithm}[H]\label{alg:layerm}

		\Fn{SingleLayer}{

			\KwIn{individual, magnitude}
			\KwOut{individual}

			layer $=$ RandomChoice(size(individual),$1$)\;
			change $=$ individual[layer] * magnitude * RandomNormal()\;
			compensation $=$ change/(size(individual)-$1$)\;
			individual[layer] $=$ individual[layer] + change\;
			\For{$i = 0$ \KwTo \textit{size(individual)}}{
				\If{$i$ $\neq$ $layer$}{
					individual[i] $=$ individual[i] $-$ compensation\;
				}
			}
			\KwRet individual\;
		}

		\caption{Single Layer mutation}

	\end{algorithm}

	\vspace{0.3cm}

	\begin{algorithm}[H]\label{alg:scalem}

		\Fn{UniformScale}{

			\KwIn{individual, magnitude}
			\KwOut{individual}

			sign $=$ $1$ \textbf{if} Random() $\leq$ $0.5$ \textbf{else} $-1$\;
			change $=$ Sum(individual) * magnitude / size(individual)\;
			\For{$i = 0$ \KwTo \textit{size(individual)}}{
				individual[i] $=$ individual[i] + change * sign\;
			}
			\KwRet individual\;
		}

		\caption{Uniform Scaling mutation}

	\end{algorithm}

\section{Training optimization}

	Now that we presumably have an idea of what sort of structure our neural network needs, it is time to take a further step to make the most of it. Since in structure optimization the remaining hyperparameters are chosen to speed up training times, it is unlikely that they already have the best values for full performance.

	Let us take a closer look at the hyperparameters of this section:

	\begin{itemize}

		\item
		Learning rate: it dictates the fraction of the measured error that is used to correct the network (thus being between 0 and 1). A small one makes the training slower but more reliable, and a high one does the opposite. It is fundamental to find a value that allows a fast training without skipping over promising error minima.

		\item
		Number of epochs: it determines how many weight readjustments are performed before the training stops. Too few cause underfitting, while too many can produce overfitting.

		\item
		Dropout rate: it is a technique that disables a portion of the neurons (randomly selected) at each training epoch. As a result, it is a regularization method because the different sub-models that try to learn from the data have less predictive power. We need to tune it so that the final neural network is neither too powerful nor too simple for the task.

	\end{itemize}

	An individual in this algorithm will have a value for each of these three fields; for clarity, we will call them \texttt{ind.lr}, \texttt{ind.epochs} and \texttt{ind.dropout}, respectively.

	Like we did in last section, we will use Algorithms, \ref{alg:evaluation}, \ref{alg:nds}, \ref{alg:fronts}, \ref{alg:crowding_distance}, \ref{alg:selection} and \ref{alg:replacement}. Additionally, we want mutations to happen independently from crossovers, so we can use Algorithm \ref{alg:offspring_nn} too. The NSGA-II framework (Algorithm \ref{alg:nsga}) is modified to receive the maximum number of epochs and the ranges for the other two hyperparameters.

	The initialization in this case is fairly trivial, as seen in Algorithm \ref{alg:lo_initialization}.

	\vspace{0.3cm}

	\begin{algorithm}[H]\label{alg:lo_initialization}

		\Fn{Initialize}{

			\KwIn{population size, max epochs, lr range, dropout range}
			\KwOut{population}
			population $\longleftarrow$ $\emptyset$\;
			\For{$i = 0$ \KwTo $population$ $size$}{
				ind.lr = Random(lr range)\;
				ind.dropout = Random(dropout range)\;
				ind.epochs = RandomChoice(1, max epochs, 1)\;
				population $\longleftarrow$ population $\cup$ $\{$ind$\}$\;
			}
			\KwRet population\;
		}

		\caption{Initialization in learning optimization}

	\end{algorithm}
