\chapter{Neural network optimization}\label{ch:optimization}
%************************************************ 

The definition of hyperparameter is often fuzzy. In this work, we view hyperparameters as those parameters which are fixed before training a model, because there are no direct rules to infer them from the available data. In fact, we probably would not need them at all if we had enough data, since the samples would tell us everything that is to be known about a given problem. As we will almost never find ourselves in such a favorable situation, there exists a real need to find the right combination of hyperparameters for the circumstances at hand.

One could understand feature selection as a form of hyperparameter optimization, since it is in a way fixing a part of the model---the entry point of the data. Although it is undoubtedly a key tool when dealing with an absurd quantity of decision variables, whether it is hyperparameter optimization or not, it already has a section in its own right. From here on, we will focus on tuning aspects specific to neural networks, such as the overall structure, the learning rate, the number of epochs in training or the dropout rate \cite{srivastava2014dropout}.

Perhaps the most common technique for this task is the \textit{grid search}. In grid search, we manually specify a set of values for each hyperparameter, and then the algorithm tries every possible combination. While efficient in low-dimensional spaces, it can also suffer from the curse of dimensionality. However, it is \textit{embarrassingly parallel} in exchange, for the combinations are independent from one another.

When the above method becomes too resource-intensive, random search \cite{bergstra2012random} comes into play. It was recently shown that a random search is capable of achieving similar results within a fraction of the time otherwise consumed by a grid equivalent; furthermore, it could yield even better outcomes if granted the same computation power.

Finally, if we look at quality-guided searches, we find classic algorithms such as \textit{simulated annealing} \cite{pai2005support}, \textit{particle swarm optimization} \cite{lin2008particle} or, again, genetic algorithms \cite{leung2003tuning}.

For scope constraints we will make use of genetic algorithms as our main optimization technique, carrying on our \acs{NSGA-II} implementation from the feature selection phase. In general, it is computationally more expensive than random search---future work could tackle the problem from this point of view---, but in turn it has the genetic operators to leverage.

In the following two sections we will describe two separate optimizations: first, we will find an approximate structure (hidden layers and their sizes); then, we will tune the learning and dropout rates and the number of training epochs. The reason for this is that their nature is different---the former deals with a variable number of parameters that define a whole, whereas the latter comprises exactly three parameters that are not conceptually linked together. Besides, joining these two parts would entail an overly convoluted search process.

Now, let us begin without further delay. We will reuse the \acs{NSGA-II} framework (Algorithm \ref{alg:nsga}), as well as the components explained in Algorithms \ref{alg:evaluation}, \ref{alg:nds}, \ref{alg:fronts}, \ref{alg:crowding_distance}, \ref{alg:selection} and \ref{alg:replacement}.
