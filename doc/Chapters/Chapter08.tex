\chapter{Conclusions and future work}\label{ch:conclusions}
%************************************************  

\section{Conclusions}

	\subsection{Software developed}

		At the end of this work we have a fully functional codebase \cite{githubrepo} that is theoretically able to address any machine learning problem of the same nature as the one we have attempted to solve here; this means that we can perform a solid feature selection making use of well-established genetic operators and then pass the resulting features on to a two-step neural network optimization method of similar structure if the problem demands so.

		The software is also able to leverage the implicit parallelism in GPUs via \texttt{TensorFlow} and the explicit process distribution to multiple CPU cores and threads, which---only lacking minor technical tweaks---makes for a scalable workflow that can be reused for larger datasets.

		Additionally, the code is written in a modern language like \texttt{Python} and uses popular and up-to-date machine learning and deep learning libraries, which facilitates code maintenance, extensibility and longevity due to the wide community support.

		We can safely state that we have accomplished the purpose of this work, since through the different optimization steps we have been able to choose in a non-arbitrary manner several hyperparameters found in neural networks: the structure (number of inputs and composition of the hidden layers) and three learning parameters (namely, the learning and dropout rates and the number of training epochs). Moreover, the process of designing and writing the necessary code has led to a superior understanding of how the involved technologies work, which was the remaining goal.

	\subsection{The problem tackled in this work}

		The dataset pertains to the area of \acs{BCI}, and in particular it is aimed at telling apart the imagined movements of the left hand, the right hand and the feet. Decisive advances in this classification task have the potential to be useful in medical applications.

		If we assume the accuracy obtained in this work as correct, these results unfold optimistic prospects in terms of practical applications. Although more detailed research is still needed, we can at least say this much. What is more, we have witnessed how \acs{SVM}s---which are not very intricate both in concept and in training---are capable of fulfilling the classification task to an outstanding level.

		Using neural networks in this dataset is perhaps not the most efficient option. Nonetheless, it has allowed us to try the neural network optimization method and conclude that it is powerful enough to be useful.

\section{Future work}

	During the progress of this project many opportunities for further work have appeared. For clarity, we list the most important ones below:

	\begin{itemize}

		\item
		Find the right feature limit in feature selection. Currently, the algorithm tends to use almost as many features as it is allowed; this raises the question of where to put the cap in order to maximize accuracy without losing generalization to unseen data (including the test set).

		\item
		Perform detailed testing of the use of cross-validation against the use of simplicity in the structure optimization phase.

		\item
		Tune the SVM hyperparameter $C$ in order to improve even more its current top accuracy. For this matter we could develop a modified version of the neural network learning optimization or we could use other techniques such as grid search or random search.

		\item
		Try more ambitious learning optimization setups (that is, more individuals and more generations) in order to find out if it yields better models.

		\item
		Find out if there is the possibility of feature transfer between different subjects. This would allow us to skip a big part of the process and make a possible real-world application easier to carry out.

	\end{itemize}

	Less relevant open issues include a better control over randomness in the code or making CPU parallelism a little more robust to increases in the scale of the experiments. In the direction of the latter, an analysis of energy consumption could be conducted, since---as we have seen---parallel computation is of great value when we need to perform statistical analyses on multiple experiment runs.
