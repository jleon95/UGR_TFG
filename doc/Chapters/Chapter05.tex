\chapter{Feature selection}\label{ch:featureselection}
%************************************************

Like we mentioned before, the chosen approach for the feature selection step is a genetic algorithm. With a few modifications specific to our problem, the basic structure will be that of \acs{NSGA-II}.

We previously hinted in chapter \ref{ch:background} at a binary representation when dealing with this kind of task. It not only facilitates the overall implementation, but---as we already said---it also brings with it some easy yet effective operators.

\section{Feature selection procedure}

	The main body of the algorithm corresponds to a typical \acs{NSGA-II} layout. We will see its general form before going over the different parts:

	\vspace{0.3cm}

	\begin{algorithm}[H]

		\Proc{NSGA-II}{

			\KwData{population size, generations, data, max features}
			\KwResult{final population}
			population $\longleftarrow$ Initialize(population size, max features)\;
			evaluation $\longleftarrow$ Evaluate(population, data)\;
			sorting $\longleftarrow$ Sort(population, evaluation)\;
			
			\For{$gen = 0$ \KwTo $max$ $generations$}{

				parents $\longleftarrow$ Selection(population, sorting)\;
				offspring $\longleftarrow$ CreateOffspring(parents)\;
				shared population $\longleftarrow$ population $\cup$ offspring\;
				evaluation $\longleftarrow$ Evaluate(shared population, data)\;
				sorting $\longleftarrow$ Sort(shared population, evaluation)\;
				population $\longleftarrow$ KeepBestN(shared population, sorting)\;
			}

			\KwRet population
		}

		\caption{NSGA-II}

	\end{algorithm}


