\chapter{Feature selection}\label{ch:featureselection}
%************************************************

Like we mentioned before, the chosen approach for the feature selection step is a genetic algorithm. With a few modifications specific to our problem, the basic structure will be that of \acs{NSGA-II}.

We previously hinted in chapter \ref{ch:background} at a binary representation when dealing with this kind of task. It not only facilitates the overall implementation, but---as we already said---it also brings with it some easy yet effective operators.

\section{Feature selection procedure}

	The main body of the algorithm corresponds to a typical \acs{NSGA-II} layout. Let us see its general form before going over the different parts:

	\vspace{0.3cm}

	\begin{algorithm}[H]

		\Proc{NSGA-II}{

			\KwIn{population size, generations, data, max features}
			\KwOut{final population}
			population $\longleftarrow$ Initialize(population size, max features)\;
			evaluation $\longleftarrow$ Evaluate(population, data)\;
			sorting $\longleftarrow$ Sort(population, evaluation)\;

			\For{$gen = 0$ \KwTo $max$ $generations$}{

				parents $\longleftarrow$ Selection(population, sorting)\;
				offspring $\longleftarrow$ CreateOffspring(parents)\;
				shared population $\longleftarrow$ population $\cup$ offspring\;
				evaluation $\longleftarrow$ Evaluate(shared population, data)\;
				sorting $\longleftarrow$ Sort(shared population, evaluation)\;
				population $\longleftarrow$ Replace(shared population, sorting)\;
			}

			\KwRet population
		}

		\caption{NSGA-II}

	\end{algorithm}

	\vspace{0.3cm}

	Notice that it returns the whole population from the last generation. Ideally, we want to take its first Pareto front and choose whatever solution we deem more appropriate for our needs.

	We can proceed to go now into more detail about the different functions that make up the body of the algorithm.

\newpage

	At the beginning, there is a randomized initialization of the population, so as to have something to start with:

	\vspace{0.3cm}

	\begin{algorithm}[H]

		\Fn{Initialize}{

			\KwIn{population size, max features}
			\KwOut{population}
			population $\longleftarrow$ $\emptyset$\;
			\For{$i = 0$ \KwTo $population$ $size$}{
				population $\longleftarrow$ population $\cup$ RandomVector(max features)\;
			}

			\KwRet population
		}

		\caption{Population initialization}

	\end{algorithm}

	\vspace{0.3cm}

	In the actual code, each element of the population is created as a sequence of zeros which is then modified to introduce ones in random positions.

	After that, and also after each time we create a new population, we have to evaluate the fitness of its individuals. This is accomplished in \texttt{Evaluate}:

	\vspace{0.3cm}

	\begin{algorithm}[H]

		\Fn{Evaluate}{

			\KwIn{population, data}
			\KwOut{evaluation}
			evaluation $\longleftarrow$ EmptyMatrix(population size, objective count)

			\For{$obj = 0$ \KwTo $objective$ $count$}{

				\For{$ind = 0$ \KwTo $population$ $size$}{

					evaluation[ind][obj] $\longleftarrow$ $F_{obj}$(population[ind], data)
				}
			}

			\KwRet evaluation
		}

		\caption{Population evaluation}

	\end{algorithm}

	\vspace{0.3cm}

	Here we assume that we already have the different $F_i$ at our disposal. Also, not all fitness functions necessarily use the data for their computations---we will discuss this soon---but it is written this way for uniformity.
